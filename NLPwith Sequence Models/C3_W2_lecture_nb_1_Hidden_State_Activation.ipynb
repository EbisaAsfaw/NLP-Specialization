{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EbisaAsfaw/NLP-Specialization/blob/main/NLPwith%20Sequence%20Models/C3_W2_lecture_nb_1_Hidden_State_Activation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCkXTjSYuUZa"
      },
      "source": [
        "# Hidden State Activation : Ungraded Lecture Notebook\n",
        "\n",
        "In this notebook you'll take another look at the hidden state activation function. It can be written in two different ways. \n",
        "\n",
        "I'll show you, step by step, how to implement each of them and then how to verify whether the results produced by each of them are same or not.\n",
        "\n",
        "## Background\n",
        "\n",
        "![vanilla rnn](images/vanilla_rnn.PNG)\n",
        "\n",
        "\n",
        "This is the hidden state activation function for a vanilla RNN.\n",
        "\n",
        "$h^{<t>}=g(W_{h}[h^{<t-1>},x^{<t>}] + b_h)$                                                    \n",
        "\n",
        "Which is another way of writing this:         \n",
        "\n",
        "$h^{<t>}=g(W_{hh}h^{<t-1>} + W_{hx}x^{<t>} + b_h)$                                        \n",
        "\n",
        "Where \n",
        "\n",
        "- $W_{h}$ in the first formula is denotes the *horizontal* concatenation of $W_{hh}$ and $W_{hx}$ from the second formula.\n",
        "\n",
        "- $W_{h}$ in the first formula is then multiplied by $[h^{<t-1>},x^{<t>}]$, another concatenation of parameters from the second formula but this time in a different direction, i.e *vertical*!\n",
        "\n",
        "Let us see what this means computationally.\n",
        "\n",
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAe4EVFmuUZh"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3KOO1YuuUZk"
      },
      "source": [
        "## Joining (Concatenation)\n",
        "\n",
        "### Weights\n",
        "\n",
        "A join along the vertical boundary is called a *horizontal concatenation* or *horizontal stack*. \n",
        "\n",
        "Visually, it looks like this:- $W_h = \\left [ W_{hh} \\ | \\ W_{hx} \\right ]$\n",
        "\n",
        "I'll show you two different ways to achieve this using numpy.\n",
        "\n",
        "__Note: The values used to populate the arrays, below, have been chosen to aid in visual illustration only. They are NOT what you'd expect to use building a model, which would typically be random variables instead.__\n",
        "\n",
        "* Try using random initializations for the weight arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "u6bdok43uUZl",
        "outputId": "d0054770-2436-4a72-bd1d-898363d036a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Data --\n",
            "\n",
            "w_hh :\n",
            "[[ 1.14375846 -0.35621821]\n",
            " [ 1.86310486 -0.27392125]\n",
            " [ 0.63566722  0.4499816 ]]\n",
            "w_hh shape : (3, 2) \n",
            "\n",
            "w_hx :\n",
            "[[ 1.46312033 -0.50889636 -0.97323053]\n",
            " [ 0.15754179  0.89853973  0.21757832]\n",
            " [ 0.0162809   0.73302021  0.32200896]]\n",
            "w_hx shape : (3, 3) \n",
            "\n",
            "-- Joining --\n",
            "\n",
            "option 1 : concatenate\n",
            "\n",
            "w_h :\n",
            "[[ 1.14375846 -0.35621821  1.46312033 -0.50889636 -0.97323053]\n",
            " [ 1.86310486 -0.27392125  0.15754179  0.89853973  0.21757832]\n",
            " [ 0.63566722  0.4499816   0.0162809   0.73302021  0.32200896]]\n",
            "w_h shape : (3, 5) \n",
            "\n",
            "option 2 : hstack\n",
            "\n",
            "w_h :\n",
            "[[ 1.14375846 -0.35621821  1.46312033 -0.50889636 -0.97323053]\n",
            " [ 1.86310486 -0.27392125  0.15754179  0.89853973  0.21757832]\n",
            " [ 0.63566722  0.4499816   0.0162809   0.73302021  0.32200896]]\n",
            "w_h shape : (3, 5)\n"
          ]
        }
      ],
      "source": [
        "# Create some dummy data\n",
        "\n",
        "w_hh = np.full((3, 2), 1)  # illustration purposes only, returns an array of size 3x2 filled with all 1s\n",
        "w_hx = np.full((3, 3), 9)  # illustration purposes only, returns an array of size 3x3 filled with all 9s\n",
        "\n",
        "\n",
        "### START CODE HERE ###\n",
        "# Try using some random initializations, though it will obfuscate the join. eg: uncomment these lines\n",
        "w_hh = np.random.standard_normal((3,2))\n",
        "w_hx = np.random.standard_normal((3,3))\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(\"-- Data --\\n\")\n",
        "print(\"w_hh :\")\n",
        "print(w_hh)\n",
        "print(\"w_hh shape :\", w_hh.shape, \"\\n\")\n",
        "print(\"w_hx :\")\n",
        "print(w_hx)\n",
        "print(\"w_hx shape :\", w_hx.shape, \"\\n\")\n",
        "\n",
        "# Joining the arrays\n",
        "print(\"-- Joining --\\n\")\n",
        "# Option 1: concatenate - horizontal\n",
        "w_h1 = np.concatenate((w_hh, w_hx), axis=1)\n",
        "print(\"option 1 : concatenate\\n\")\n",
        "print(\"w_h :\")\n",
        "print(w_h1)\n",
        "print(\"w_h shape :\", w_h1.shape, \"\\n\")\n",
        "\n",
        "# Option 2: hstack\n",
        "w_h2 = np.hstack((w_hh, w_hx))\n",
        "print(\"option 2 : hstack\\n\")\n",
        "print(\"w_h :\")\n",
        "print(w_h2)\n",
        "print(\"w_h shape :\", w_h2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H87r2xYvuUZn"
      },
      "source": [
        "### Hidden State & Inputs\n",
        "Joining along a horizontal boundary is called a vertical concatenation or vertical stack. Visually it looks like this:\n",
        "\n",
        "$[h^{<t-1>},x^{<t>}] = \\left[ \\frac{h^{<t-1>}}{x^{<t>}} \\right]$\n",
        "\n",
        "\n",
        "I'll show you two different ways to achieve this using numpy.\n",
        "\n",
        "*Try using random initializations for the hiddent state and input matrices.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "r2z5_ifBuUZo",
        "outputId": "8e4be537-59bf-477e-d7c9-194ecf81231f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Data --\n",
            "\n",
            "h_t_prev :\n",
            "[[-0.5219985 ]\n",
            " [-0.81932448]]\n",
            "h_t_prev shape : (2, 1) \n",
            "\n",
            "x_t :\n",
            "[[ 0.24536588]\n",
            " [ 0.71276894]\n",
            " [-0.31911915]]\n",
            "x_t shape : (3, 1) \n",
            "\n",
            "-- Joining --\n",
            "\n",
            "option 1 : concatenate\n",
            "\n",
            "ax_1 :\n",
            "[[-0.5219985 ]\n",
            " [-0.81932448]\n",
            " [ 0.24536588]\n",
            " [ 0.71276894]\n",
            " [-0.31911915]]\n",
            "ax_1 shape : (5, 1) \n",
            "\n",
            "option 2 : vstack\n",
            "\n",
            "ax_2 :\n",
            "[[-0.5219985 ]\n",
            " [-0.81932448]\n",
            " [ 0.24536588]\n",
            " [ 0.71276894]\n",
            " [-0.31911915]]\n",
            "ax_2 shape : (5, 1)\n"
          ]
        }
      ],
      "source": [
        "# Create some more dummy data\n",
        "h_t_prev = np.full((2, 1), 1)  # illustration purposes only, returns an array of size 2x1 filled with all 1s\n",
        "x_t = np.full((3, 1), 9)       # illustration purposes only, returns an array of size 3x1 filled with all 9s\n",
        "\n",
        "# Try using some random initializations, though it will obfuscate the join. eg: uncomment these lines\n",
        "\n",
        "### START CODE HERE ###\n",
        "h_t_prev = np.random.standard_normal((2,1))\n",
        "x_t = np.random.standard_normal((3,1))\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(\"-- Data --\\n\")\n",
        "print(\"h_t_prev :\")\n",
        "print(h_t_prev)\n",
        "print(\"h_t_prev shape :\", h_t_prev.shape, \"\\n\")\n",
        "print(\"x_t :\")\n",
        "print(x_t)\n",
        "print(\"x_t shape :\", x_t.shape, \"\\n\")\n",
        "\n",
        "# Joining the arrays\n",
        "print(\"-- Joining --\\n\")\n",
        "\n",
        "# Option 1: concatenate - vertical\n",
        "ax_1 = np.concatenate(\n",
        "    (h_t_prev, x_t), axis=0\n",
        ")  # note the difference in axis parameter vs earlier\n",
        "print(\"option 1 : concatenate\\n\")\n",
        "print(\"ax_1 :\")\n",
        "print(ax_1)\n",
        "print(\"ax_1 shape :\", ax_1.shape, \"\\n\")\n",
        "\n",
        "# Option 2: vstack\n",
        "ax_2 = np.vstack((h_t_prev, x_t))\n",
        "print(\"option 2 : vstack\\n\")\n",
        "print(\"ax_2 :\")\n",
        "print(ax_2)\n",
        "print(\"ax_2 shape :\", ax_2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72DH-iwRuUZp"
      },
      "source": [
        "## Verify Formulas\n",
        "Now you know how to do the concatenations, horizontal and vertical, lets verify if the two formulas produce the same result.\n",
        "\n",
        "__Formula 1:__ $h^{<t>}=g(W_{h}[h^{<t-1>},x^{<t>}] + b_h)$ \n",
        "\n",
        "__Formula 2:__ $h^{<t>}=g(W_{hh}h^{<t-1>} + W_{hx}x^{<t>} + b_h)$\n",
        "\n",
        "\n",
        "To prove:- __Formula 1__ $\\Leftrightarrow$ __Formula 2__\n",
        "\n",
        "We will ignore the bias term $b_h$ and the activation function $g(\\ )$ because the transformation will be identical for each formula. So what we really want to compare is the result of the following parameters inside each formula:\n",
        "\n",
        "$W_{h}[h^{<t-1>},x^{<t>}] \\quad \\Leftrightarrow \\quad W_{hh}h^{<t-1>} + W_{hx}x^{<t>} $\n",
        "\n",
        "We'll see how to do this using matrix multiplication combined with the data and techniques (stacking/concatenating) from above.\n",
        "\n",
        "* Try adding a sigmoid activation function and bias term to the checks for completeness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "NJ2PlWueuUZr",
        "outputId": "38d9d2f2-25d8-4aa2-c066-9b98b7b00225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Results --\n",
            "\n",
            "Formula 1\n",
            "Term1:\n",
            " [[-0.88555392  0.34212863 -1.46912505 -0.57877507  1.50645044]\n",
            " [-0.39068958 -1.45558001  0.16685353  0.076849   -0.77949071]\n",
            " [-0.86396937 -0.68383042 -1.25323741 -0.75314168  0.72307055]]\n",
            "Term2:\n",
            " [[-0.31474372]\n",
            " [ 1.41235895]\n",
            " [-0.32482897]\n",
            " [-0.62938162]\n",
            " [ 0.24370387]]\n",
            "Output:\n",
            "[[ 1.97054353]\n",
            " [-2.22536547]\n",
            " [ 0.36343135]]\n",
            "\n",
            "Formula 2\n",
            "Term1:\n",
            " [[ 0.76193096]\n",
            " [-1.93283437]\n",
            " [-0.69388509]]\n",
            "Term2:\n",
            " [[ 1.20861257]\n",
            " [-0.29253111]\n",
            " [ 1.05731644]]\n",
            "\n",
            "Output:\n",
            "[[ 1.97054353]\n",
            " [-2.22536547]\n",
            " [ 0.36343135]] \n",
            "\n",
            "-- Verify --\n",
            "Results are the same : True\n"
          ]
        }
      ],
      "source": [
        "# Data\n",
        "\n",
        "w_hh = np.full((3, 2), 1)  # returns an array of size 3x2 filled with all 1s\n",
        "w_hx = np.full((3, 3), 9)  # returns an array of size 3x3 filled with all 9s\n",
        "h_t_prev = np.full((2, 1), 1)  # returns an array of size 2x1 filled with all 1s\n",
        "x_t = np.full((3, 1), 9)       # returns an array of size 3x1 filled with all 9s\n",
        "\n",
        "\n",
        "# If you want to randomize the values, uncomment the next 4 lines\n",
        "\n",
        "w_hh = np.random.standard_normal((3,2))\n",
        "w_hx = np.random.standard_normal((3,3))\n",
        "h_t_prev = np.random.standard_normal((2,1))\n",
        "x_t = np.random.standard_normal((3,1))\n",
        "\n",
        "# Results\n",
        "print(\"-- Results --\")\n",
        "# Formula 1\n",
        "stack_1 = np.hstack((w_hh, w_hx))\n",
        "stack_2 = np.vstack((h_t_prev, x_t))\n",
        "\n",
        "print(\"\\nFormula 1\")\n",
        "print(\"Term1:\\n\",stack_1)\n",
        "print(\"Term2:\\n\",stack_2)\n",
        "formula_1 = np.matmul(np.hstack((w_hh, w_hx)), np.vstack((h_t_prev, x_t)))\n",
        "print(\"Output:\")\n",
        "print(formula_1)\n",
        "\n",
        "# Formula 2\n",
        "mul_1 = np.matmul(w_hh, h_t_prev)\n",
        "mul_2 = np.matmul(w_hx, x_t)\n",
        "print(\"\\nFormula 2\")\n",
        "print(\"Term1:\\n\",mul_1)\n",
        "print(\"Term2:\\n\",mul_2)\n",
        "\n",
        "formula_2 = np.matmul(w_hh, h_t_prev) + np.matmul(w_hx, x_t)\n",
        "print(\"\\nOutput:\")\n",
        "print(formula_2, \"\\n\")\n",
        "\n",
        "# Verification \n",
        "# np.allclose - to check if two arrays are elementwise equal upto certain tolerance, here  \n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.allclose.html\n",
        "\n",
        "print(\"-- Verify --\")\n",
        "print(\"Results are the same :\", np.allclose(formula_1, formula_2))\n",
        "\n",
        "### START CODE HERE ###\n",
        "# # Try adding a sigmoid activation function and bias term as a final check\n",
        "# # Activation\n",
        "# def sigmoid(x):\n",
        "#     return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# # Bias and check\n",
        "# b = np.random.standard_normal((formula_1.shape[0],1))\n",
        "# print(\"Formula 1 Output:\\n\",sigmoid(formula_1+b))\n",
        "# print(\"Formula 2 Output:\\n\",sigmoid(formula_2+b))\n",
        "\n",
        "# all_close = np.allclose(sigmoid(formula_1+b), sigmoid(formula_2+b))\n",
        "# print(\"Results after activation are the same :\",all_close)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clL6BkJHuUZs"
      },
      "source": [
        "## Summary\n",
        "That's it! We've verified that the two formulas produce the same results, and seen how to combine matrices vertically and horizontally to make that happen. We now have all the intuition needed to understand the math notation of RNNs."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "C3_W2_lecture_nb_1_Hidden_State_Activation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}